{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pypdf langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"/home/mahya/Desktop/MissionAssistant/documents/Kiel/geomar/ifm-geomar_rep3.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFM-GEOMAR REPORT Berichte aus dem Leibniz-Institut \n",
      "f√ºr Meereswissenschaften an der \n",
      "Christian-Albr\n",
      "{'source': '/home/mahya/Desktop/MissionAssistant/documents/Kiel/geomar/ifm-geomar_rep3.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_chroma in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: langchain_openai in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (0.2.12)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_chroma) (0.5.3)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_chroma) (0.110.1)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_chroma) (0.3.25)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_chroma) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_openai) (1.57.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.2)\n",
      "Requirement already satisfied: requests>=2.28 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.6.6)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.66.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (30.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.37.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (0.2.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1.40->langchain_chroma) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: idna>=2.8 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.1)\n",
      "Requirement already satisfied: certifi in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain_chroma) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.34.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain_chroma) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.25.3)\n",
      "Requirement already satisfied: sympy in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from requests>=2.28->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.20.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
      "Requirement already satisfied: filelock in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.20.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.17.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_chroma langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysqlite3-binary in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (0.5.3.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pysqlite3-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f79c5f83df0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **RV SONNE**\\n- **RV Poseidon**'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. list your answer in between two asterisks that each response in bullet list\" # limit your answer to 1 sentence\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "\"\"\"\n",
    "Q1\n",
    "\"\"\"\n",
    "results = rag_chain.invoke({\"input\": \"What is vehicle/ vessel name?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*I don't know.*\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q2\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is vehicle vessel type?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- *The main purpose of the cruise SO 176 \"MERAMEX I\" was to install an array of ocean bottom seismic stations to monitor the seismicity of the Java subduction zone and to collect high resolution bathymetric data during transit.*'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q3\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the main purpose of the cruise report?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **The duration of cruise S0176 was less than three days, from May 18 to May 21, 2004.**\\n- **The duration of cruise S0179 was 20 days, from September 17 to October 6, 2004.**'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q4\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the duration of the mission?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, I can identify the following tasks related to the mission:\\n\\n- **Retrieval of Deep-Sea Observation System (DOS):** The retrieval took place under ideal weather conditions on August 10, 2004. The task involved establishing acoustic contact with the release system, triggering the release command, and recovering the floating lander. This task was successful as the recovery went smoothly, and the systems worked well, including successful sediment sampling.\\n\\nUnfortunately, the provided context does not specify the duration of each task or details about other tasks related to the mission. If there are additional tasks, their durations, and whether they were successful or failures is not mentioned in the given text.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q5\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What are existing tasks in this mission, the duration of each task, \\\n",
    "                            and whether the task was successful or failure?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tasks in this mission include:\\n\\n- Conducting CTD/Ro transects at Kiel Mount and the Belgica Mound Province\\n- Retrieval of the Deep-Sea Observation System (DOS)\\n- Sediment sampling and processing\\n- Studying fauna and taxonomy, focusing on some biological aspects related to POS316\\n\\n**Please let me know if you need further details on any of these tasks.**'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q6\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What are all the tasks in this mission?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided information, the mission utilized the following components and devices:\\n\\n- **Gravity Sensor**: Included a spring-mass assembly, control electronics, and a power supply with a 24-hour battery backup.\\n- **Leveling Subsystem**: Composed of a platform stabilized by an electrically erected gyro, with control electronics and power supply located in the data handling subsystem.\\n- **Magnetometer**: Operated in single sensor mode, towed 300 meters aft of the vessel, and deployed at varying depths depending on speed.\\n- **ADCP (Acoustic Doppler Current Profiler)**: Two models, a 300 kHz ADCP measuring every 15 minutes in 3-meter cells up to 110 meters above the bottom, and a 1200 kHz ADCP measuring in 10-cm cells within the first 100 cm of the sediment-water interface.\\n- **Optical Sensors**: Included a transmissiometer, fluorometer, and optical backscatter sensor, deployed 50 cm from the seafloor.\\n- **Acoustic Current Meter and Turbidity Meter**: Mounted next to the sediment trap.\\n- **Sediment Trap**: Sampled particle deposition in 8-day intervals.\\n- **Colonization Substrates**: Various natural hard and soft substrates for colonization experiments, including limestone, basalt, granite pebbles, and coral fragments.\\n\\nThe document does not specify the exact duration for each component's operation beyond the 24-hour power supply backup for the gravity sensor and the 8-day sampling intervals for the sediment trap. The mission elements were operational during the deployment and recovery of the DOS lander, which occurred between April 2004 and August 2004.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q7\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What components/devices/sensors used in this mission and for how long?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The application of the MERAMEX project is to investigate seismological and tectonic processes in the Sunda subduction zone, particularly focusing on the seismic coupling zone, stress distribution, and subduction-related arc volcanism. The project involves setting up a seismological network to monitor seismicity, resolve structural images from seismic events, and understand the relationship between subduction zone processes and arc volcanism, especially around the Merapi volcano. Additionally, the project aims to gather data that could help in understanding the dynamics of the seismic gap south of Java and its implications for regional seismic risk.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q8\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the application of the mission?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **The mission location for POSEIDON Cruise No. 316 is in the Icelandic EEZ, starting at 61¬∞37‚ÄôN; 22¬∞48‚ÄôW.**'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q9\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the mission location?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q10\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the visibility of the environment and how to measure it?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **The temperature of the Surface Water (SW) decreases from 13.5¬∞C to 11¬∞C as depth increases. Below the thermocline, the temperature of the North Atlantic Central Water (NACW) decreases continuously.**'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Q13\n",
    "\"\"\"\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is the temperature of water in depth?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(results[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m**([^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m*]*)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mresults\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m match:\n\u001b[1;32m      5\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.findall(r'\\**([^\\*]*)\\**', results[\"answer\"])\n",
    "\n",
    "for item in match:\n",
    "   if len(item)>3:\n",
    "      print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 16, 'source': '../documents/Kiel/geomar/ifm-geomar_rep33.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there is no specific information available about a mission named \"anton 89\". Therefore, I cannot provide a detailed step-by-step explanation of this mission.\n",
      "\n",
      "**However, here are some related points from the context that can provide a general understanding of AUV missions:**\n",
      "\n",
      "* **AUV Mapping Techniques and Results:**\n",
      "  - Four AUV dives collected bathymetric data.\n",
      "  - One additional dive collected sidescan sonar data.\n",
      "  - Low frequency (200 kHz) bathymetry data and sidescan sonar data processed using CARAIBES package from IFREMER.\n",
      "  - High-frequency (400 kHz) bathymetric data processed using PDS2000 by RESON.\n",
      "\n",
      "* **Dive 012 Example:**\n",
      "  - The target was a small volcano near the ridge axis of segment 1B.\n",
      "  - Previous data suggested a turbidity anomaly in the water column.\n",
      "  - Due to highly variable topography, the mission was designed in two parts.\n",
      "\n",
      "* **AUV Technical Description:**\n",
      "  - The AUV ABYSS, built by HYDROID from IFM-GEOMAR, operates in depths up to 6000 m.\n",
      "  - The ABYSS system includes the AUV, control and workshop container, and a mobile Launch and Recovery System (LARS).\n",
      "  - LARS, developed by WHOI, supports ship-based operations, negating the need for a Zodiac for launch/recovery.\n",
      "  - The AUV can be deployed and recovered from either port or starboard side of medium and large German research vessels.\n",
      "  - LARS is stored in a 20 ft. container during transport.\n",
      "  - Deployment and recovery are feasible in weather conditions with a swell.\n",
      "\n",
      "If you have more specific details or another context about \"anton 89,\" please provide them, and I'd be happy to assist further.\n"
     ]
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\": \"explain auv mission anton 89 step by step in detail?\"})\n",
    "print(results[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**I don't know.**\n"
     ]
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\": \"the path of anton 89 with specific locations step by step in detail?\"})\n",
    "print(results[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.findall(r'\\**([^\\*]*)\\**', results[\"answer\"])\n",
    "\n",
    "for item in match:\n",
    "   if len(item)>3:\n",
    "      print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'\n",
    "sentence_orange = 'Oranges are my favorite fruit'\n",
    "sentences = [sentence_obama, sentence_president, sentence_orange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mahya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "a = preprocess(sentences[0])\n",
    "b = preprocess(sentences[1])\n",
    "c = preprocess(sentences[2])\n",
    "\n",
    "sentence_cleaned = [a, b, c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 13:53:49,506 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:49,507 : INFO : built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\n",
      "2024-10-16 13:53:49,508 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\", 'datetime': '2024-10-16T13:53:49.508221', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 1.0175\n"
     ]
    }
   ],
   "source": [
    "# !pip install pot\n",
    "\n",
    "distance = model.wmdistance(sentence_cleaned[0], sentence_cleaned[1])\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 13:53:53,126 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,127 : INFO : built Dictionary<4 unique tokens: ['illinois', 'media', 'obama', 'speaks']> from 2 documents (total 8 corpus positions)\n",
      "2024-10-16 13:53:53,127 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<4 unique tokens: ['illinois', 'media', 'obama', 'speaks']> from 2 documents (total 8 corpus positions)\", 'datetime': '2024-10-16T13:53:53.127952', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,129 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,129 : INFO : built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\n",
      "2024-10-16 13:53:53,130 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...> from 2 documents (total 8 corpus positions)\", 'datetime': '2024-10-16T13:53:53.130352', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,131 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,132 : INFO : built Dictionary<7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...> from 2 documents (total 7 corpus positions)\n",
      "2024-10-16 13:53:53,132 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...> from 2 documents (total 7 corpus positions)\", 'datetime': '2024-10-16T13:53:53.132448', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,135 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,136 : INFO : built Dictionary<8 unique tokens: ['chicago', 'greets', 'president', 'press', 'illinois']...> from 2 documents (total 8 corpus positions)\n",
      "2024-10-16 13:53:53,137 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<8 unique tokens: ['chicago', 'greets', 'president', 'press', 'illinois']...> from 2 documents (total 8 corpus positions)\", 'datetime': '2024-10-16T13:53:53.137046', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,138 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,139 : INFO : built Dictionary<4 unique tokens: ['chicago', 'greets', 'president', 'press']> from 2 documents (total 8 corpus positions)\n",
      "2024-10-16 13:53:53,139 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<4 unique tokens: ['chicago', 'greets', 'president', 'press']> from 2 documents (total 8 corpus positions)\", 'datetime': '2024-10-16T13:53:53.139648', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,141 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,142 : INFO : built Dictionary<7 unique tokens: ['chicago', 'greets', 'president', 'press', 'favorite']...> from 2 documents (total 7 corpus positions)\n",
      "2024-10-16 13:53:53,142 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<7 unique tokens: ['chicago', 'greets', 'president', 'press', 'favorite']...> from 2 documents (total 7 corpus positions)\", 'datetime': '2024-10-16T13:53:53.142632', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,144 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,145 : INFO : built Dictionary<7 unique tokens: ['favorite', 'fruit', 'oranges', 'illinois', 'media']...> from 2 documents (total 7 corpus positions)\n",
      "2024-10-16 13:53:53,145 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<7 unique tokens: ['favorite', 'fruit', 'oranges', 'illinois', 'media']...> from 2 documents (total 7 corpus positions)\", 'datetime': '2024-10-16T13:53:53.145769', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,147 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,148 : INFO : built Dictionary<7 unique tokens: ['favorite', 'fruit', 'oranges', 'chicago', 'greets']...> from 2 documents (total 7 corpus positions)\n",
      "2024-10-16 13:53:53,148 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<7 unique tokens: ['favorite', 'fruit', 'oranges', 'chicago', 'greets']...> from 2 documents (total 7 corpus positions)\", 'datetime': '2024-10-16T13:53:53.148906', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n",
      "2024-10-16 13:53:53,150 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-10-16 13:53:53,151 : INFO : built Dictionary<3 unique tokens: ['favorite', 'fruit', 'oranges']> from 2 documents (total 6 corpus positions)\n",
      "2024-10-16 13:53:53,151 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<3 unique tokens: ['favorite', 'fruit', 'oranges']> from 2 documents (total 6 corpus positions)\", 'datetime': '2024-10-16T13:53:53.151576', 'gensim': '4.3.3', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-5.15.0-91-generic-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0174646858929572\n",
      "1.3663502993722163\n",
      "1.0174646858929575\n",
      "0.0\n",
      "1.3388266063724354\n",
      "1.3663502993722163\n",
      "1.3388266063724352\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i, s1 in enumerate(sentences):\n",
    "   for j, s2 in enumerate(sentences):\n",
    "      wmd_score = model.wmdistance(sentence_cleaned[i], sentence_cleaned[j])\n",
    "      print(wmd_score)\n",
    "      # res[s1].append(round(distance, 1))\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: torch in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mahya/anaconda3/envs/env1/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load CodeBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code1 = \"\"\"\n",
    "(define (domain rv_poseidon)\n",
    "\n",
    "  (:requirements :strips :typing :fluents :durative-actions)\n",
    "\n",
    "  (:types \n",
    "    sensor task)\n",
    "\n",
    "  (:predicates \n",
    "    (motion_data_collected ?sensor - sensor)\n",
    "    (gps_position_acquired ?sensor - sensor)\n",
    "    (ctd_measurement_taken ?sensor - sensor)\n",
    "    (data_analyzed ?task - task)\n",
    "  )\n",
    "\n",
    "  (:functions \n",
    "    (data_quality ?task - task) ; Represents the quality of the data collected from sensors\n",
    "  )\n",
    "\n",
    "  ;; Durative action to collect motion data using the Motion Sensor IXSEA OCTANS 1000\n",
    "  (:durative-action collect_motion_data\n",
    "    :parameters (?sensor - sensor)\n",
    "    :duration (= ?duration 15)\n",
    "    :condition (and (at start (not (motion_data_collected ?sensor))))\n",
    "    :effect (and (at end (motion_data_collected ?sensor))\n",
    "                 (at end (increase (data_quality motion_task) 5)))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to acquire GPS position using the GPS-Receiver GARMIN 152\n",
    "  (:durative-action acquire_gps_position\n",
    "    :parameters (?sensor - sensor)\n",
    "    :duration (= ?duration 10)\n",
    "    :condition (and (at start (not (gps_position_acquired ?sensor))))\n",
    "    :effect (and (at end (gps_position_acquired ?sensor))\n",
    "                 (at end (increase (data_quality gps_task) 3)))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to take CTD measurements using the CTD48M Sound Velocity Probe\n",
    "  (:durative-action take_ctd_measurement\n",
    "    :parameters (?sensor - sensor)\n",
    "    :duration (= ?duration 20)\n",
    "    :condition (and (at start (not (ctd_measurement_taken ?sensor))))\n",
    "    :effect (and (at end (ctd_measurement_taken ?sensor))\n",
    "                 (at end (increase (data_quality ctd_task) 7)))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to analyze collected data after gathering motion, GPS, and CTD data\n",
    "  (:durative-action analyze_data\n",
    "    :parameters (?task - task)\n",
    "    :duration (= ?duration 30)\n",
    "    :condition (and (at start (motion_data_collected ixsea_octans_1000))\n",
    "                    (at start (gps_position_acquired garmin_152))\n",
    "                    (at start (ctd_measurement_taken ctd48m)))\n",
    "    :effect (at end (data_analyzed ?task))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings1 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code2 = \"\"\"\n",
    "(define (domain sopran_project)\n",
    "\n",
    "  (:requirements :strips :typing :fluents :durative-actions)\n",
    "\n",
    "  (:types \n",
    "    station sample profile)\n",
    "\n",
    "  (:predicates \n",
    "    (ctd_profile_conducted ?station - station)\n",
    "    (water_sample_collected ?station - station ?depth - number)\n",
    "    (data_analyzed ?profile - profile)\n",
    "    (halogen_compound_variation_studied ?profile - profile)\n",
    "    (hydrographic_conditions_understood ?profile - profile)\n",
    "  )\n",
    "\n",
    "  (:functions \n",
    "    (depth ?station - station) ; Represents the depth of the water sample collection\n",
    "  )\n",
    "\n",
    "  ;; Durative action to conduct a CTD profile\n",
    "  (:durative-action conduct_ctd_profile\n",
    "    :parameters (?station - station)\n",
    "    :duration (= ?duration 15)\n",
    "    :condition (and (at start (not (ctd_profile_conducted ?station))))\n",
    "    :effect (and (at start (ctd_profile_conducted ?station)))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to collect water samples at different depths using Niskin bottles\n",
    "  (:durative-action collect_water_sample\n",
    "    :parameters (?station - station ?depth - number)\n",
    "    :duration (= ?duration 10)\n",
    "    :condition (and (at start (ctd_profile_conducted ?station))\n",
    "                    (at start (not (water_sample_collected ?station ?depth))))\n",
    "    :effect (and (at end (water_sample_collected ?station ?depth)))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to analyze the collected data for temperature, salinity, and oxygen levels\n",
    "  (:durative-action analyze_data\n",
    "    :parameters (?profile - profile)\n",
    "    :duration (= ?duration 20)\n",
    "    :condition (at start (not (data_analyzed ?profile)))\n",
    "    :effect (at end (data_analyzed ?profile))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to study halogenated compound variations\n",
    "  (:durative-action study_halogen_compounds\n",
    "    :parameters (?profile - profile)\n",
    "    :duration (= ?duration 30)\n",
    "    :condition (at start (data_analyzed ?profile))\n",
    "    :effect (at end (halogen_compound_variation_studied ?profile))\n",
    "  )\n",
    "\n",
    "  ;; Durative action to understand hydrographic conditions\n",
    "  (:durative-action understand_hydrographic_conditions\n",
    "    :parameters (?profile - profile)\n",
    "    :duration (= ?duration 25)\n",
    "    :condition (at start (data_analyzed ?profile))\n",
    "    :effect (at end (hydrographic_conditions_understood ?profile))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings2 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code3 = \"\"\"\n",
    "(define (domain NAMV_operations)\n",
    "  (:requirements :strips :durative-actions)\n",
    "  \n",
    "  ;; Types of tasks\n",
    "  (:types task observatory instrument ship CTD OBS)\n",
    "\n",
    "  ;; Predicates\n",
    "  (:predicates\n",
    "    (data_downloaded ?obs)\n",
    "    (CTD_performed ?ctd)\n",
    "    (microstructure_tested ?instrument)\n",
    "    (methane_plume_identified ?method)\n",
    "    (OBS_recovered ?obs)\n",
    "    (equipment_tested ?instrument)\n",
    "    (temperature_observatory_recovered ?obs)\n",
    "    (task_completed ?task)\n",
    "    (task_failed ?task))\n",
    "\n",
    "  ;; Action for downloading data\n",
    "  (:durative-action download_data\n",
    "    :parameters (?obs - observatory)\n",
    "    :duration (= ?duration 2)\n",
    "    :condition (and (at start (not (data_downloaded ?obs))))\n",
    "    :effect (and (at end (data_downloaded ?obs) (task_completed download_data)))\n",
    "  )\n",
    "\n",
    "  ;; Action for performing CTD cast\n",
    "  (:durative-action perform_CTD_cast\n",
    "    :parameters (?ctd - CTD)\n",
    "    :duration (= ?duration 2)\n",
    "    :condition (and (at start (not (CTD_performed ?ctd))))\n",
    "    :effect (and (at end (CTD_performed ?ctd) (task_completed perform_CTD_cast)))\n",
    "  )\n",
    "\n",
    "  ;; Action for testing temperature microstructure\n",
    "  (:durative-action test_microstructure\n",
    "    :parameters (?instrument - instrument)\n",
    "    :duration (= ?duration 1)\n",
    "    :condition (and (at start (not (microstructure_tested ?instrument))))\n",
    "    :effect (at end (task_failed test_microstructure))\n",
    "  )\n",
    "\n",
    "  ;; Action for identifying methane plume\n",
    "  (:durative-action identify_methane_plume\n",
    "    :parameters (?method - ship)\n",
    "    :duration (= ?duration 4)\n",
    "    :condition (and (at start (not (methane_plume_identified ?method))))\n",
    "    :effect (and (at end (methane_plume_identified ?method) (task_completed identify_methane_plume)))\n",
    "  )\n",
    "\n",
    "  ;; Action for recovering OBS stations\n",
    "  (:durative-action recover_OBS\n",
    "    :parameters (?obs - OBS)\n",
    "    :duration (= ?duration 4)\n",
    "    :condition (and (at start (not (OBS_recovered ?obs))))\n",
    "    :effect (and (at end (OBS_recovered ?obs) (task_completed recover_OBS)))\n",
    "  )\n",
    "\n",
    "  ;; Action for ROV deployment and equipment test\n",
    "  (:durative-action deploy_ROV\n",
    "    :parameters (?instrument - instrument)\n",
    "    :duration (= ?duration 3.5)\n",
    "    :condition (and (at start (not (equipment_tested ?instrument))))\n",
    "    :effect (and (at end (equipment_tested ?instrument) (task_completed deploy_ROV)))\n",
    "  )\n",
    "\n",
    "  ;; Action for recovering temperature observatory\n",
    "  (:durative-action recover_temperature_observatory\n",
    "    :parameters (?obs - observatory)\n",
    "    :duration (= ?duration 8)\n",
    "    :condition (and (at start (not (temperature_observatory_recovered ?obs))))\n",
    "    :effect (and (at end (temperature_observatory_recovered ?obs) (task_completed recover_temperature_observatory)))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code3, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings3 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code4 = \"\"\"\n",
    "(define (domain auv_anton_mission)\n",
    "\n",
    "  (:requirements :strips :typing :fluents :durative-actions)\n",
    "\n",
    "  (:types \n",
    "    auv setting)\n",
    "\n",
    "  (:predicates \n",
    "    (mission_started ?auv - auv)\n",
    "    (mission_completed ?auv - auv)\n",
    "    (acoustic_command_tested ?auv - auv)\n",
    "    (drift_tested ?auv - auv)\n",
    "    (no_response ?auv - auv)\n",
    "    (usbl_received ?auv - auv)\n",
    "    (settings_changed ?auv - auv)\n",
    "  )\n",
    "\n",
    "  (:functions \n",
    "    (depth ?auv - auv) ; Represents the current depth of the AUV\n",
    "    (duration ?auv - auv) ; Represents the duration of the mission in minutes\n",
    "  )\n",
    "\n",
    "  ;; Action to start a mission with an acoustic GoTo command and handle drift\n",
    "  (:durative-action acoustic_goto_command\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 10)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 200)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (drift_tested ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action to test acoustic commands\n",
    "  (:durative-action test_acoustic_command\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 14)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 20)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (acoustic_command_tested ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action for drift testing\n",
    "  (:durative-action perform_drift_test\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 29)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 50)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (drift_tested ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action to test acoustic abort command with no response, then change settings\n",
    "  (:durative-action test_acoustic_abort_command\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 6)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 20)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (no_response ?auv))\n",
    "                 (at end (settings_changed ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action to test acoustic commands with new settings, resulting in no response\n",
    "  (:durative-action test_acoustic_command_new_settings\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 8)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 20))\n",
    "                    (at start (settings_changed ?auv)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (no_response ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action to test acoustic commands again with new settings, resulting in no response\n",
    "  (:durative-action test_acoustic_command_new_settings_again\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 7)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 20))\n",
    "                    (at start (settings_changed ?auv)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (no_response ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    "\n",
    "  ;; Action for successful communication with USBL\n",
    "  (:durative-action receive_usbl_signal\n",
    "    :parameters (?auv - auv)\n",
    "    :duration (= ?duration 4)\n",
    "    :condition (and (at start (not (mission_started ?auv)))\n",
    "                    (at start (= (depth ?auv) 20))\n",
    "                    (at start (settings_changed ?auv)))\n",
    "    :effect (and (at start (mission_started ?auv))\n",
    "                 (at end (usbl_received ?auv))\n",
    "                 (at end (mission_completed ?auv)))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code4, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings4 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code5 = \"\"\"\n",
    "(define (domain mission-report)\n",
    "  (:requirements :strips :typing)\n",
    "  \n",
    "  (:types \n",
    "    task - object\n",
    "    duration - number\n",
    "    outcome - object\n",
    "  )\n",
    "  \n",
    "  (:predicates\n",
    "    (task-started ?t - task)\n",
    "    (task-completed ?t - task)\n",
    "    (task-failed ?t - task)\n",
    "    (task-duration ?t - task ?d - duration)\n",
    "    (task-outcome ?t - task ?o - outcome)\n",
    "  )\n",
    "\n",
    "  (:action start-seismic-profiling\n",
    "    :parameters ()\n",
    "    :precondition (not (task-started seismic-profiling))\n",
    "    :effect (and\n",
    "      (task-started seismic-profiling)\n",
    "      (task-duration seismic-profiling duration-24h)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action complete-seismic-profiling\n",
    "    :parameters ()\n",
    "    :precondition (task-started seismic-profiling)\n",
    "    :effect (and\n",
    "      (task-completed seismic-profiling)\n",
    "      (task-outcome seismic-profiling outcome-successful-recovery)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action start-heat-flow-measurements\n",
    "    :parameters ()\n",
    "    :precondition (not (task-started heat-flow-measurements))\n",
    "    :effect (and\n",
    "      (task-started heat-flow-measurements)\n",
    "      (task-duration heat-flow-measurements duration-multiple)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action complete-heat-flow-measurements\n",
    "    :parameters ()\n",
    "    :precondition (task-started heat-flow-measurements)\n",
    "    :effect (and\n",
    "      (task-completed heat-flow-measurements)\n",
    "      (task-outcome heat-flow-measurements outcome-partial-completion)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action start-magnetometer-profiles\n",
    "    :parameters ()\n",
    "    :precondition (not (task-started magnetometer-profiles))\n",
    "    :effect (and\n",
    "      (task-started magnetometer-profiles)\n",
    "      (task-duration magnetometer-profiles duration-transit)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action complete-magnetometer-profiles\n",
    "    :parameters ()\n",
    "    :precondition (task-started magnetometer-profiles)\n",
    "    :effect (and\n",
    "      (task-completed magnetometer-profiles)\n",
    "      (task-outcome magnetometer-profiles outcome-successful-data-collection)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action start-seismic-profile-P03\n",
    "    :parameters ()\n",
    "    :precondition (not (task-started seismic-profile-P03))\n",
    "    :effect (and\n",
    "      (task-started seismic-profile-P03)\n",
    "      (task-duration seismic-profile-P03 duration-shortened)\n",
    "    )\n",
    "  )\n",
    "\n",
    "  (:action complete-seismic-profile-P03\n",
    "    :parameters ()\n",
    "    :precondition (task-started seismic-profile-P03)\n",
    "    :effect (and\n",
    "      (task-completed seismic-profile-P03)\n",
    "      (task-outcome seismic-profile-P03 outcome-partial-completion)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code5, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings5 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code6 = \"\"\"\n",
    "(define (domain celtic_explorer_mission)\n",
    "  (:requirements :strips :typing)\n",
    "  (:types\n",
    "    vessel component task location\n",
    "  )\n",
    "\n",
    "  (:predicates\n",
    "    (at ?v - vessel ?l - location)               ; Vessel's current location\n",
    "    (has_component ?v - vessel ?c - component)   ; Vessel has a specific component\n",
    "    (task_completed ?t - task)                    ; Task has been completed\n",
    "    (task_in_progress ?t - task)                  ; Task is currently in progress\n",
    "    (is_successful ?t - task)                     ; Task was successful\n",
    "    (is_installed ?c - component)                 ; Component is installed\n",
    "    (is_rigged ?c - component)                    ; Component is rigged\n",
    "    (is_pumped ?c - component)                    ; Component has been pumped\n",
    "    (is_tested ?c - component)                    ; Component has been tested\n",
    "  )\n",
    "\n",
    "  (:action unpack_and_install\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v shore) (has_component ?v ?c) (not (is_installed ?c)))\n",
    "    :effect (and (is_installed ?c) (task_completed unpacking_installation))\n",
    "  )\n",
    "\n",
    "  (:action rig_up_vibro_corer\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v shore) (has_component ?v ?c) (not (is_rigged ?c)))\n",
    "    :effect (and (is_rigged ?c) (task_completed rigging_vibro_corer))\n",
    "  )\n",
    "\n",
    "  (:action conduct_meeting\n",
    "    :parameters (?v - vessel)\n",
    "    :precondition (at ?v shore)\n",
    "    :effect (task_completed meeting_principal_investigators)\n",
    "  )\n",
    "\n",
    "  (:action pump_hydraulic_oil\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v offshore) (has_component ?v ?c) (not (is_pumped ?c)))\n",
    "    :effect (and (is_pumped ?c) (task_completed pumping_oil))\n",
    "  )\n",
    "\n",
    "  (:action perform_harbor_test\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v offshore) (has_component ?v ?c) (not (is_tested ?c)))\n",
    "    :effect (and (is_tested ?c) (is_successful perform_harbor_test))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code6, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings6 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your PDDL code\n",
    "pddl_code7 = \"\"\"\n",
    "(define (domain cruise_so190_leg2)\n",
    "  (:requirements :strips :typing)\n",
    "  (:types\n",
    "    vessel component task location\n",
    "  )\n",
    "\n",
    "  (:predicates\n",
    "    (at ?v - vessel ?l - location)               ; Vessel's current location\n",
    "    (has_component ?v - vessel ?c - component)   ; Vessel has a specific component\n",
    "    (task_completed ?t - task)                    ; Task has been completed\n",
    "    (task_in_progress ?t - task)                  ; Task is currently in progress\n",
    "    (is_successful ?t - task)                     ; Task was successful\n",
    "    (is_deployed ?c - component)                  ; Component is deployed\n",
    "    (is_recovered ?c - component)                 ; Component has been recovered\n",
    "    (is_mapped ?t - task)                         ; Task of mapping is completed\n",
    "    (is_transiting ?v - vessel)                   ; Vessel is transiting\n",
    "  )\n",
    "\n",
    "  (:action conduct_bathymetric_survey\n",
    "    :parameters (?v - vessel)\n",
    "    :precondition (at ?v pacific_ocean)\n",
    "    :effect (and (task_completed bathymetric_survey) (is_successful bathymetric_survey))\n",
    "  )\n",
    "\n",
    "  (:action deploy_instruments\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v pacific_ocean) (has_component ?v ?c) (not (is_deployed ?c)))\n",
    "    :effect (and (is_deployed ?c) (task_completed deploying_instruments))\n",
    "  )\n",
    "\n",
    "  (:action recover_instruments\n",
    "    :parameters (?v - vessel ?c - component)\n",
    "    :precondition (and (at ?v pacific_ocean) (has_component ?v ?c) (is_deployed ?c))\n",
    "    :effect (and (is_recovered ?c) (task_completed recovering_instruments))\n",
    "  )\n",
    "\n",
    "  (:action map_trench_and_slope\n",
    "    :parameters (?v - vessel)\n",
    "    :precondition (at ?v pacific_ocean)\n",
    "    :effect (and (is_mapped trench_and_slope_mapping) (task_completed mapping_trench_slope))\n",
    "  )\n",
    "\n",
    "  (:action initiate_transit\n",
    "    :parameters (?v - vessel)\n",
    "    :precondition (and (at ?v pacific_ocean) (not (is_transiting ?v)))\n",
    "    :effect (and (is_transiting ?v) (task_completed initiating_transit))\n",
    "  )\n",
    ")\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(pddl_code7, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings7 = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "py_code1 = \"\"\"\n",
    "def QA_LangChain():\n",
    "    '''\n",
    "    This code is relared to question answering of gpt4 model using RAG chain.\n",
    "    '''\n",
    "    results = rag_chain.invoke({\"input\": \"What is the visibility of the environment and how to measure it?\"})\n",
    "    return results['answer']\n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(py_code1, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings1p = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n",
    "embeddings1p = outputs.pooler_output  # Mean pooling over token embeddings for a single vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "py_code2 = \"\"\"\n",
    "  (define (domain cruise_so190_leg2)\n",
    "    (:requirements :strips :typing)\n",
    "    (:types\n",
    "      vessel component task location\n",
    "    )\n",
    "\n",
    "    (:predicates\n",
    "      (at ?v - vessel ?l - location)               ; Vessel's current location\n",
    "      (has_component ?v - vessel ?c - component)   ; Vessel has a specific component\n",
    "      (task_completed ?t - task)                    ; Task has been completed\n",
    "      (task_in_progress ?t - task)                  ; Task is currently in progress\n",
    "      (is_successful ?t - task)                     ; Task was successful\n",
    "      (is_deployed ?c - component)                  ; Component is deployed\n",
    "      (is_recovered ?c - component)                 ; Component has been recovered\n",
    "      (is_mapped ?t - task)                         ; Task of mapping is completed\n",
    "      (is_transiting ?v - vessel)                   ; Vessel is transiting\n",
    "    )\n",
    "\n",
    "    (:action conduct_bathymetric_survey\n",
    "      :parameters (?v - vessel)\n",
    "      :precondition (at ?v pacific_ocean)\n",
    "      :effect (and (task_completed bathymetric_survey) (is_successful bathymetric_survey))\n",
    "    )\n",
    "\n",
    "    (:action deploy_instruments\n",
    "      :parameters (?v - vessel ?c - component)\n",
    "      :precondition (and (at ?v pacific_ocean) (has_component ?v ?c) (not (is_deployed ?c)))\n",
    "      :effect (and (is_deployed ?c) (task_completed deploying_instruments))\n",
    "    )\n",
    "\n",
    "    (:action recover_instruments\n",
    "      :parameters (?v - vessel ?c - component)\n",
    "      :precondition (and (at ?v pacific_ocean) (has_component ?v ?c) (is_deployed ?c))\n",
    "      :effect (and (is_recovered ?c) (task_completed recovering_instruments))\n",
    "    )\n",
    "\n",
    "    (:action map_trench_and_slope\n",
    "      :parameters (?v - vessel)\n",
    "      :precondition (at ?v pacific_ocean)\n",
    "      :effect (and (is_mapped trench_and_slope_mapping) (task_completed mapping_trench_slope))\n",
    "    )\n",
    "\n",
    "    (:action initiate_transit\n",
    "      :parameters (?v - vessel)\n",
    "      :precondition (and (at ?v pacific_ocean) (not (is_transiting ?v)))\n",
    "      :effect (and (is_transiting ?v) (task_completed initiating_transit))\n",
    "    )\n",
    "  )\n",
    " \n",
    "\"\"\"\n",
    "# Tokenize input PDDL text\n",
    "inputs = tokenizer(py_code2, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Pass tokenized inputs through CodeBERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtain the embedding vector (use the last hidden state or pooler output)\n",
    "embeddings2p = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over token embeddings for a single vector\n",
    "embeddings2p = outputs.pooler_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embedding_list1 = embeddings1.detach().cpu().numpy().tolist()\n",
    "embedding_list1[0]\n",
    "\n",
    "embedding_list2 = embeddings2.detach().cpu().numpy().tolist()\n",
    "embedding_list2[0]\n",
    "\n",
    "embedding_list3 = embeddings3.detach().cpu().numpy().tolist()\n",
    "embedding_list3[0]\n",
    "\n",
    "embedding_list4 = embeddings4.detach().cpu().numpy().tolist()\n",
    "embedding_list4[0]\n",
    "\n",
    "\n",
    "embedding_list5 = embeddings5.detach().cpu().numpy().tolist()\n",
    "embedding_list5[0]\n",
    "\n",
    "embedding_list6 = embeddings6.detach().cpu().numpy().tolist()\n",
    "embedding_list6[0]\n",
    "\n",
    "embedding_list7 = embeddings7.detach().cpu().numpy().tolist()\n",
    "embedding_list7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_list1p = embeddings1p.detach().cpu().numpy().tolist()\n",
    "# print(np.size(embedding_list1p))\n",
    "\n",
    "embedding_list2p = embeddings2p.detach().cpu().numpy().tolist()\n",
    "# print(np.size(embedding_list2p))\n",
    "\n",
    "embed1p =embedding_list1p[0]\n",
    "\n",
    "print(np.size(embed1p))\n",
    "\n",
    "embed2p =embedding_list2p[0]\n",
    "\n",
    "print(np.size(embed2p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "embed1 =embedding_list1[0]\n",
    "\n",
    "np.size(embed1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed2 =embedding_list2[0]\n",
    "np.size(embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed3 =embedding_list3[0]\n",
    "np.size(embed3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed4 =embedding_list4[0]\n",
    "np.size(embed4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed5 =embedding_list5[0]\n",
    "np.size(embed5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed6 =embedding_list6[0]\n",
    "np.size(embed6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed7 =embedding_list7[0]\n",
    "np.size(embed7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein Distance: 0.00888292147866802\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "distance = wasserstein_distance(embed1p, embed2p)\n",
    "print(\"Wasserstein Distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Distance: 2.3021702526313472\n"
     ]
    }
   ],
   "source": [
    "l2_distance = np.linalg.norm(np.array(embed1p) - np.array(embed2p))\n",
    "print(\"L2 Distance:\", l2_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.971894878429664\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity = np.dot(np.array(embed1p), np.array(embed2p)) / (np.linalg.norm(np.array(embed1p)) \n",
    "                                                                  * np.linalg.norm(np.array(embed2p)))\n",
    "print(\"Cosine Similarity:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': 0.8734881, 'recall': 0.81292915, 'f1': 0.8421212758259986}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Load CodeBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def get_token_embeddings(code_snippet):\n",
    "    # Tokenize the code and get token embeddings\n",
    "    inputs = tokenizer(code_snippet, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden_size)\n",
    "\n",
    "def cosine_similarity_matrix(embedding1, embedding2):\n",
    "    # Compute pairwise cosine similarities between tokens in two sequences\n",
    "    embedding1 = embedding1.cpu().numpy()\n",
    "    embedding2 = embedding2.cpu().numpy()\n",
    "    return 1 - pairwise_distances(embedding1, embedding2, metric=\"cosine\")\n",
    "\n",
    "def bert_score(code1, code2):\n",
    "    # Get token embeddings for each code snippet\n",
    "    embeddings1 = get_token_embeddings(code1)\n",
    "    embeddings2 = get_token_embeddings(code2)\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity_matrix(embeddings1, embeddings2)\n",
    "    \n",
    "    # Precision: for each token in code1, find max similarity in code2\n",
    "    precision = np.mean(np.max(sim_matrix, axis=1))\n",
    "    \n",
    "    # Recall: for each token in code2, find max similarity in code1\n",
    "    recall = np.mean(np.max(sim_matrix, axis=0))\n",
    "    \n",
    "    # F1 Score: harmonic mean of precision and recall\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1_score}\n",
    "\n",
    "def meteor_score_codebert(code1, code2):\n",
    "    # Get token embeddings\n",
    "    embeddings1 = get_token_embeddings(code1)\n",
    "    embeddings2 = get_token_embeddings(code2)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity_matrix(embeddings1, embeddings2)\n",
    "    \n",
    "    # Exact and synonym matches based on similarity threshold (e.g., >= 0.8 for synonym match)\n",
    "    threshold = 0.8\n",
    "    matched1 = (sim_matrix >= threshold).any(axis=1).sum()  # Matches in code1\n",
    "    matched2 = (sim_matrix >= threshold).any(axis=0).sum()  # Matches in code2\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = matched1 / len(embeddings1)\n",
    "    recall = matched2 / len(embeddings2)\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    # Penalty for unmatched sequences\n",
    "    penalty = 0.5 * ((1 - precision) + (1 - recall))\n",
    "    \n",
    "    # METEOR Score with penalty\n",
    "    meteor = f1_score * (1 - penalty)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1_score\": f1_score, \"meteor_score\": meteor}\n",
    "\n",
    "\n",
    "def rouge_score_codebert(code1, code2, threshold=0.8):\n",
    "    # Get token embeddings for each code snippet\n",
    "    embeddings1 = get_token_embeddings(code1)\n",
    "    embeddings2 = get_token_embeddings(code2)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = cosine_similarity_matrix(embeddings1, embeddings2)\n",
    "    \n",
    "    # ROUGE-1 (Unigram) Calculation\n",
    "    matched1 = (sim_matrix >= threshold).any(axis=1).sum()  # Matches in code1\n",
    "    matched2 = (sim_matrix >= threshold).any(axis=0).sum()  # Matches in code2\n",
    "    rouge1_precision = matched1 / len(embeddings1)\n",
    "    rouge1_recall = matched2 / len(embeddings2)\n",
    "    rouge1_f1 = 2 * (rouge1_precision * rouge1_recall) / (rouge1_precision + rouge1_recall + 1e-10)  # Small value to prevent div by zero\n",
    "    \n",
    "    # ROUGE-2 (Bigram) Calculation - create bigrams by combining consecutive token embeddings\n",
    "    bigrams1 = [embeddings1[i:i+2] for i in range(len(embeddings1) - 1)]\n",
    "    bigrams2 = [embeddings2[i:i+2] for i in range(len(embeddings2) - 1)]\n",
    "    bigram_matches = sum(\n",
    "        max(cosine_similarity_matrix(b1, b2).max() >= threshold for b2 in bigrams2)\n",
    "        for b1 in bigrams1\n",
    "    )\n",
    "    rouge2_precision = bigram_matches / len(bigrams1) if bigrams1 else 0\n",
    "    rouge2_recall = bigram_matches / len(bigrams2) if bigrams2 else 0\n",
    "    rouge2_f1 = 2 * (rouge2_precision * rouge2_recall) / (rouge2_precision + rouge2_recall + 1e-10)\n",
    "    \n",
    "    # # ROUGE-L Calculation - longest common subsequence based on similarity\n",
    "    # lcs_len = sum(1 for i, row in enumerate(sim_matrix) if row.max() >= threshold)\n",
    "    # rouge_l_precision = lcs_len / len(embeddings1)\n",
    "    # rouge_l_recall = lcs_len / len(embeddings2)\n",
    "    # rouge_l_f1 = 2 * (rouge_l_precision * rouge_l_recall) / (rouge_l_precision + rouge_l_recall + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        \"ROUGE-1\": {\"precision\": rouge1_precision, \"recall\": rouge1_recall, \"f1\": rouge1_f1},\n",
    "        \"ROUGE-2\": {\"precision\": rouge2_precision, \"recall\": rouge2_recall, \"f1\": rouge2_f1},\n",
    "        # \"ROUGE-L\": {\"precision\": rouge_l_precision, \"recall\": rouge_l_recall, \"f1\": rouge_l_f1}\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "score = bert_score(py_code1, py_code2) #pddl_code2, pddl_code3)\n",
    "print(\"BERTScore:\", score)\n",
    "\n",
    "# score = meteor_score_codebert(code1, code2)\n",
    "# print(\"METEORScore:\", score)\n",
    "\n",
    "# score = rouge_score_codebert(code1, code2)\n",
    "# print(\"ROUGE Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
